{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define source to download\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "PRIME_LIMITS = 1000000\n",
    "PRIMES_ZIP_FILE_NAME = \"primes1.zip\"\n",
    "PRIMES_EXTRACT_FILE_NAME = \"primes1.txt\"\n",
    "PRIMES_PATH = \"datasets/primes\"\n",
    "\n",
    "def extract_primes_data(primes_path=PRIMES_PATH, primes_zip_file_name=PRIMES_ZIP_FILE_NAME, primes_extract_file_name=PRIMES_EXTRACT_FILE_NAME):\n",
    "    if not os.path.isdir(primes_path):\n",
    "        raise Exception('Create the path: datasets/primes and copy the ' + primes_zip_file_name + ' within the path')\n",
    "    zip_path = os.path.join(primes_path, primes_zip_file_name)\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise Exception('Download primes zip from URL=https://primes.utm.edu/lists/small/millions/')\n",
    "    primes_zip = zipfile.ZipFile(zip_path)\n",
    "    txt_path = os.path.join(primes_path, primes_extract_file_name)\n",
    "    with open(txt_path, 'w') as f:\n",
    "        data = primes_zip.read(primes_extract_file_name)\n",
    "        # split into line of data\n",
    "        data = data.splitlines(True)\n",
    "        # remove first lines due to header text type\n",
    "        data = data[4:]\n",
    "        new_list = []\n",
    "        # filter blank lines and reduce the result to one array\n",
    "        for line in data[:]:\n",
    "            values = line.decode(\"utf-8\").split()\n",
    "            if len(values) > 0:\n",
    "                for num in values:\n",
    "                    new_list.append(num)\n",
    "        text = \"\\n\".join(new_list)\n",
    "        f.write(text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract file\n",
    "extract_primes_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method for loading extracted data\n",
    "import pandas as pd\n",
    "\n",
    "def load_primes_data(primes_path=PRIMES_PATH, primes_extract_file_name=PRIMES_EXTRACT_FILE_NAME, limit=None):\n",
    "    csv_path = os.path.join(primes_path, primes_extract_file_name)\n",
    "    return pd.read_csv(csv_path, sep='\\t+|\\r+|\\n+|\\s+', nrows=limit, header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  5,  7, 11, 13, 17, 19, 23, 29])"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from prepared file\n",
    "primes_data = load_primes_data(limit=PRIME_LIMITS).values.flatten()\n",
    "primes_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate non primes data\n",
    "non_primes = []\n",
    "for val in range(len(primes_data)):\n",
    "    non_primes.append(val)\n",
    "\n",
    "non_primes = [x for x in non_primes if x not in primes_data]\n",
    "non_primes_data = np.array(non_primes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_primes_data[:10])\n",
    "print(non_primes_data[:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels for primes\n",
    "y_prime = (primes_data == primes_data)\n",
    "y_prime[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lables for non-primes\n",
    "y_non_prime = (non_primes_data != non_primes_data)\n",
    "y_non_prime[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_numbers = np.concatenate((primes_data, non_primes_data))\n",
    "all_labels = np.concatenate((y_prime, y_non_prime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose dataset\n",
    "data = {'values' : pd.Series(all_numbers),\n",
    "        'labels' : pd.Series(all_labels)}\n",
    "data_frame = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_frame.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(data_frame, data_frame[\"labels\"]):\n",
    "    train_set = data_frame.loc[train_index]\n",
    "    test_set = data_frame.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the stratified split\n",
    "print(\"original data proportions\\n\", data_frame[\"labels\"].value_counts() / len(data_frame))\n",
    "print(\"stratified test proportions\\n\", test_set[\"labels\"].value_counts() / len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[\"labels\"]\n",
    "X_train = train_set[\"values\"].values.reshape(-1, 1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train)\n",
    "print(\"X_train count:\", len(X_train))\n",
    "print(\"y_train count:\", len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation above using the cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# use cross-validation k-fold = 3 and measure the accuracy in percent\n",
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Classifier Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an good alternative instead of using the scores we can retrieve the cross-validation predictions \n",
    "# and build a confusion matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3)\n",
    "\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize the tradeoff of precision and recall we can use the decision_function instead of predict returning\n",
    "# scores instead of predictions\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train, cv=3, method=\"decision_function\")\n",
    "\n",
    "# calculate values for ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain with RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now trying to improve the Precision/Recall using a RandomForest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "# some classifiers do not provide a decision_function method but a dict_proba method due to their type of \n",
    "# classification -> to use the dict_proba, we parametrize the cross_val_predict with its predict_proba parameter\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train, cv=3, method=\"predict_proba\")\n",
    "\n",
    "# to plot a ROC curve we need to transform the prbabilities to scores\n",
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train, y_scores_forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(forest_clf, X_train, y_train, cv=3)\n",
    "\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC curve\n",
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
    "plt.plot(fpr_forest, tpr_forest, label=\"Random Forest\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the precisions, recalls and thresholds to plot the tradeoff\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n",
    "forest_precisions, forest_recalls, thresholds = precision_recall_curve(y_train, y_scores_forest)\n",
    "\n",
    "# plot precision vs recall\n",
    "plt.plot(recalls, precisions, \"b:\", label=\"SGD\")\n",
    "plt.plot(forest_recalls, forest_precisions, \"g-\", label=\"Random Forest\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.sort_values([\"values\"], ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.plot(kind=\"scatter\", x=\"values\", y=\"values\", alpha=0.8, s=data_frame[\"labels\"]/data_frame[\"values\"], label=\"prime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn_clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, 100, 50, 20), random_state=42)\n",
    "nn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(nn_clf, X_train, y_train, cv=3)\n",
    "\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
